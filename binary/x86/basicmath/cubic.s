
	.data
        .align 16
.mask:
        .quad 9223372036854775807
.LC0:
	.long	1073774592
	.long	0
	.long	0
	.long	0
.LC1:
	.long	1073881088
	.long	0
	.long	0
	.long	0
.LC2:
	.long	1073983488
	.long	0
	.long	0
	.long	0
.LC3:
	.long	1074049024
	.long	0
	.long	0
	.long	0
.LC4:
	.long	-1073741824
	.long	0
.LC5:
	.long	1074266112
	.long	0
.LC6:
	.long	1075388923
	.long	1413754136
.LC7:
	.long	1076437499
	.long	1413754136
.LC8:
	.long	1070945621
	.long	1431655765
.LC9:
	.long	1072693248
	.long	0
.LC10:
	.long	-1074790400
	.long	0
.text
.globl SolveCubic
SolveCubic:
xor %r12,%r12
pushq %rbp
addq $-192,%rsp
movq %rbp,160(%rsp)
movq %r13,152(%rsp)
movq %rbx,144(%rsp)
movsd %xmm11,184(%rsp)
movq %rsp,%rbp
movsd %xmm0,96(%rbp)
movsd %xmm1,104(%rbp)
movsd %xmm2,112(%rbp)
movsd %xmm3,120(%rbp)
movq %r14,128(%rbp)
movq %r8,136(%rbp)
movsd 104(%rbp),%xmm5
movsd 96(%rbp),%xmm4
movapd %xmm5,%xmm15
divsd %xmm4,%xmm15
movapd %xmm15,%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,8(%rbp)
movq %rax,0(%rbp)
movsd 112(%rbp),%xmm5
movsd 96(%rbp),%xmm4
movapd %xmm5,%xmm15
divsd %xmm4,%xmm15
movapd %xmm15,%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,24(%rbp)
movq %rax,16(%rbp)
movsd 120(%rbp),%xmm5
movsd 96(%rbp),%xmm4
movapd %xmm5,%xmm15
divsd %xmm4,%xmm15
movapd %xmm15,%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,40(%rbp)
movq %rax,32(%rbp)
movsd 8(%rbp),%xmm3
movsd 0(%rbp),%xmm2
movsd 8(%rbp),%xmm1
movsd 0(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC0(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movsd 24(%rbp),%xmm1
movsd 16(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%rsi
movq %rax,%rdi
leaq .LC1(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movq %rsi,%xmm1
movq %rdi,%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,56(%rbp)
movq %rax,48(%rbp)
movq 8(%rbp),%r11
movq 0(%rbp),%rax
movsd 8(%rbp),%xmm3
movsd 0(%rbp),%xmm2
movq %r11,%xmm1
movq %rax,%xmm0
call __addtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movsd 8(%rbp),%xmm3
movsd 0(%rbp),%xmm2
movq %r11,%xmm1
movq %rax,%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movsd 8(%rbp),%xmm3
movsd 0(%rbp),%xmm2
movq %r11,%xmm1
movq %rax,%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC1(%rip),%rax
movsd 0(%rax),%xmm2
movsd 8(%rax),%xmm3
movsd 8(%rbp),%xmm1
movsd 0(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movsd 24(%rbp),%xmm3
movsd 16(%rbp),%xmm2
movq %r11,%xmm1
movq %rax,%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC2(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movsd 40(%rbp),%xmm1
movsd 32(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __addtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%rsi
movq %rax,%rdi
leaq .LC3(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movq %rsi,%xmm1
movq %rdi,%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,72(%rbp)
movq %rax,64(%rbp)
movsd 72(%rbp),%xmm3
movsd 64(%rbp),%xmm2
movsd 72(%rbp),%xmm1
movsd 64(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
movsd 56(%rbp),%xmm3
movsd 48(%rbp),%xmm2
movsd 56(%rbp),%xmm1
movsd 48(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movsd 56(%rbp),%xmm3
movsd 48(%rbp),%xmm2
movq %r11,%xmm1
movq %rax,%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm1
movq %rax,%xmm0
call __trunctfdf2
movsd %xmm4,80(%rbp)
movsd 80(%rbp),%xmm4
movq %r12,%xmm5
comisd %xmm5,%xmm4
jbe .L10
nop
movq 128(%rbp),%rax
movl $3,%r11d
movl %r11d,0(%rax)
movsd 56(%rbp),%xmm3
movsd 48(%rbp),%xmm2
movsd 56(%rbp),%xmm1
movsd 48(%rbp),%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movsd 56(%rbp),%xmm3
movsd 48(%rbp),%xmm2
movq %r11,%xmm1
movq %rax,%xmm0
call __multf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm1
movq %rax,%xmm0
call __trunctfdf2
movq %xmm4,%rax
movq %rax,%xmm0
call sqrt
movq %xmm4,%rax
movq %rax,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movsd 72(%rbp),%xmm1
movsd 64(%rbp),%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm1
movq %rax,%xmm0
call __trunctfdf2
movq %xmm4,%rax
movq %rax,%xmm0
call acos
movsd %xmm4,88(%rbp)
movsd 56(%rbp),%xmm1
movsd 48(%rbp),%xmm0
call __trunctfdf2
movq %xmm4,%rax
movq %rax,%xmm0
call sqrt
movsd %xmm4,%xmm5
movsd .LC4(%rip),%xmm4
movq %xmm4,%xmm11
mulsd %xmm5,%xmm11
movsd 88(%rbp),%xmm5
movsd .LC5(%rip),%xmm4
movapd %xmm5,%xmm15
divsd %xmm4,%xmm15
movapd %xmm15,%xmm4
movsd %xmm4,%xmm0
call cos
movq %xmm4,%xmm4
mulsd %xmm11,%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC0(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movsd 8(%rbp),%xmm1
movsd 0(%rbp),%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm1
movq %rax,%xmm0
call __trunctfdf2
movq 136(%rbp),%rax
movsd %xmm4,0(%rax)
movsd 56(%rbp),%xmm1
movsd 48(%rbp),%xmm0
call __trunctfdf2
movq %xmm4,%rax
movq %rax,%xmm0
call sqrt
movsd %xmm4,%xmm5
movsd .LC4(%rip),%xmm4
movq %xmm4,%xmm11
mulsd %xmm5,%xmm11
movsd 88(%rbp),%xmm5
movsd .LC6(%rip),%xmm4
addsd %xmm4,%xmm5
movsd .LC5(%rip),%xmm4
movapd %xmm5,%xmm15
divsd %xmm4,%xmm15
movapd %xmm15,%xmm4
movsd %xmm4,%xmm0
call cos
movq %xmm4,%xmm4
mulsd %xmm11,%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC0(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movsd 8(%rbp),%xmm1
movsd 0(%rbp),%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%rsi
movq %rax,%rdi
movq 136(%rbp),%rax
movq %rax,%rbx
addq $8,%rbx
movq %rsi,%xmm1
movq %rdi,%xmm0
call __trunctfdf2
movsd %xmm4,0(%rbx)
movsd 56(%rbp),%xmm1
movsd 48(%rbp),%xmm0
call __trunctfdf2
movq %xmm4,%rax
movq %rax,%xmm0
call sqrt
movsd %xmm4,%xmm5
movsd .LC4(%rip),%xmm4
movq %xmm4,%xmm11
mulsd %xmm5,%xmm11
movsd 88(%rbp),%xmm5
movsd .LC7(%rip),%xmm4
addsd %xmm4,%xmm5
movsd .LC5(%rip),%xmm4
movapd %xmm5,%xmm15
divsd %xmm4,%xmm15
movapd %xmm15,%xmm4
movsd %xmm4,%xmm0
call cos
movq %xmm4,%xmm4
mulsd %xmm11,%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC0(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movsd 8(%rbp),%xmm1
movsd 0(%rbp),%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%rsi
movq %rax,%rdi
movq 136(%rbp),%rax
movq %rax,%rbx
addq $16,%rbx
movq %rsi,%xmm1
movq %rdi,%xmm0
call __trunctfdf2
movsd %xmm4,0(%rbx)
jmp .L12
.L10:
movq 128(%rbp),%rax
movl $1,%r11d
movl %r11d,0(%rax)
movsd 80(%rbp),%xmm0
call sqrt
movsd %xmm4,%xmm11
movsd 72(%rbp),%xmm1
movsd 64(%rbp),%xmm0
call __trunctfdf2
movapd %xmm4,%xmm4
andpd .mask(%rip),%xmm4
addsd %xmm11,%xmm4
movsd .LC8(%rip),%xmm5
movsd %xmm5,%xmm1
movsd %xmm4,%xmm0
call pow
movq 136(%rbp),%rax
movsd %xmm4,0(%rax)
movq 136(%rbp),%rax
movsd 0(%rax),%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
movq 136(%rbp),%rax
movsd 0(%rax),%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movsd 56(%rbp),%xmm1
movsd 48(%rbp),%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __addtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm1
movq %rax,%xmm0
call __trunctfdf2
movq 136(%rbp),%rax
movsd %xmm4,0(%rax)
movq 136(%rbp),%rax
movsd 0(%rax),%xmm11
movq %r12,%xmm3
movq %r12,%xmm2
movsd 72(%rbp),%xmm1
movsd 64(%rbp),%xmm0
call __lttf2
cmpl %r12d,%eax
jge .L11
movsd .LC9(%rip),%xmm4
jmp .L7
.L11:
movsd .LC10(%rip),%xmm4
.L7:
movq %xmm11,%xmm4
mulsd %xmm4,%xmm4
movq 136(%rbp),%rax
movsd %xmm4,0(%rax)
movq 136(%rbp),%rax
movsd 0(%rax),%xmm4
movsd %xmm4,%xmm0
call __extenddftf2
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%r13
movq %rax,%rbx
leaq .LC0(%rip),%rax
movsd 8(%rax),%xmm3
movsd 0(%rax),%xmm2
movsd 8(%rbp),%xmm1
movsd 0(%rbp),%xmm0
call __divtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm3
movq %rax,%xmm2
movq %r13,%xmm1
movq %rbx,%xmm0
call __subtf3
movq %xmm4,%rdi
movq %xmm6,%r11
movq %rdi,%rax
movq %r11,%xmm1
movq %rax,%xmm0
call __trunctfdf2
movq 136(%rbp),%rax
movsd %xmm4,0(%rax)
.L12:
nop
movq %rbp,%rsp
movq 160(%rsp),%rbp
movq 152(%rsp),%r13
movq 144(%rsp),%rbx
movsd 184(%rsp),%xmm11
addq $192,%rsp
popq %rbp
ret
